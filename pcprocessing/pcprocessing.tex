%!TEX root = ../terrainbook.tex

\graphicspath{{pcprocessing/}}
\label{chap:pcprocessing}

\chapter{Point cloud processing}

In this lesson we discuss the basics tools and algorithms necessary for point cloud processing: storage in various file formats, thinning, outlier detection, ground filtering, and segmentation (to extract buildings or other features from a point cloud).



%%%%%%%%%%%%%%%%%%%%
%
\section{Point cloud file formats}
A point cloud is essentially a array of 3D points, and often that is also how it is stored in a file.
Regardless of the format, a point cloud file can often be seen as a array of \emph{point records}, each of which contains the coordinates and attributes of one point.
A point record consists of several \emph{fields}, each of which stores a single value, \eg\ an integer, float, or boolean.
A field can for instance represent the $x$, $y$, or $z$-coordinate of a point or one of its attributes, \eg\ the lidar return number or color information.
The order and meaning of the fields in a record is fixed for all the point records in one file.
How exactly the point records are structured and stored in the file, and what additional metadata is available, depends on the specific file format that is used.

%TODO figure + description of point record

% Notice that, in additions to the widely used formats mentioned here, there are also many proprietary formats. These are often specific to one particular software and are therefore not very useful for data exchange.

\subsection{ASCII formats}
ASCII formats are plain text files. 
The point cloud information is thus stored as a sequence of ASCII characters, usually one point record per line.
In most cases you can recognise such files by the .xyz, .csv, or .txt extension.
A benefit of ASCII files is that you can simply open them in a text editor.
The biggest downside is that they are not standardised at all, \ie\ the type, order, and number of attributes varies and also the used CRS is usually not documented in the file.

\subsection{The PLY format}
The PLY format can be considererd a standardised ASCII format.
A PLY file contains a header\footnote{A header is supplemental information placed at the beginning of a file, \eg\ to store metadata about the file.} that specifies the structure of the point records in the file, \ie\ the number of attributes (called \emph{properties}), their order, their names and their data types.
This makes it a very flexible standard, since the user can decide on the composition of the point record.
Figure~\ref{fig:ply} shows an example PLY file.
\begin{figure}
  \includegraphics[width=\linewidth]{figs/ply_header.pdf}
  \caption{A simple PLY file with 1 additional user defined attribute of type int. It contains 7 points.}
  \label{fig:ply}
\end{figure}


PLY files are readable by many software packages and can also be stored in a binary encoding\footnote{\url{https://en.wikipedia.org/wiki/PLY_(file_format)\#ASCII_or_binary_format}}.
Compared to the ASCII encoding, the binary encoding results in a smaller file size and quicker reading and writing from and to the file.
There is no standardised way to specify the CRS in a PLY file, although one could add a comment in the header that states the CRS.

\subsection{The LAS format}
The public LASER (LAS) file format is the most widely used standard for the dissemination of point cloud data.
The LAS standard is maintained by the ASPRS organisation and, as the name implies, it was designed for datasets that originate from (airborne) lidar scanners.
However, in practice it is also used for other types of point cloud, \eg\ those derived from dense image matching.
It is a binary-encoded standard and compared to the PLY format it is rather strict because it prescribes exactly what a point record should look like, \ie\ what attributes are there and how many bits each attribute should use. 

Table~\ref{tab:las-record} shows the composition of the simplest record type that is available for LAS files.
\begin{table}
  \centering
  \small
  \begin{tabular}{l|l|l|p{7cm}}
    % \toprule
    % \rowcolor[gray]{.9}
    Field & Format & Length (bits) & Description\\ \midrule
    X & int & 32 & X coordinate. \\ 
    Y & int & 32 & Y coordinate. \\ 
    Z & int & 32 & Z coordinate. \\ 
    Intensity & unsigned int & 16 & The pulse return amplitude. \\ 
    Return number & unsigned int & 3 &  The total pulse return number for a given output pulse. \\ 
    Number of returns & unsigned int & 3 & Total number of returns for a given pulse \\ 
    Scan Direction Flag & boolean & 1 & Denotes the direction at which the scanner mirror was traveling at the time of the output pulse. A bit value of 1 is a positive scan direction, and a bit value of 0 is a negative scan direction (where positive scan direction is a scan moving from the left side of the in-track direction to the right side and negative the opposite).  \\ 
    Edge of Flight Line & boolean & 1 & Has a value of 1 only when the point is at the end of a scan. It is the last point on a given scan line before it changes direction. \\ 
    Classification & unsigned int & 5 & Classification code \\ 
    Scan Angle Rank & int & 4 & The angle at which the laser pulse was output from the scanner including the roll of the aircraft. \\ 
    User Data & unsigned int & 4 & May be used at the user's discretion. \\ 
    Point Source ID & unsigned int & 8 & Indicates the file from which this point originated. Non-zero if this point was copied from another file.
    % \bottomrule
  \end{tabular}
\caption{LAS Point Data Record Format 0}
\label{tab:las-record}
\end{table}
Other record types are available that also include fields to store for instance  RGB color information or the GPS time (the time a point was measured by the scanner), but all records types include at least the fields shown in Table~\ref{tab:las-record}.
While the LAS standard clearly specifies that all these fields are required, some of the fields are very specific to lidar acquisition and they are sometimes ignored in practice, \eg\ if a point cloud originating from dense matching is stored in the LAS format. 
Notice that unused fields will still take up storage space in each record.

The CRS of the point cloud can be stored in the header of a LAS file, together with some other general information such as the total number of points and the bounding box of the point cloud. 
The X, Y, and Z fields are stored as 32-bit integers. 
To convert these values to the actual coordinates, they need to be multiplied by a scaling factor and added to an offset value, \ie:
\begin{gather*}
  X_{coordinate} = (X_{record} * X_{scale}) + X_{offset} \\
  Y_{coordinate} = (Y_{record} * Y_{scale}) + Y_{offset} \\
  Z_{coordinate} = (Z_{record} * Z_{scale}) + Z_{offset}.
\end{gather*}
The scaling factors $X_{scale}$, $Y_{scale}$, $Z_{scale}$ and the offsets $X_{offset}$, $Y_{offset}$, $Z_{offset}$ are also given in the header. Notice that the scaling factor determines the number of decimals that can be stored, \eg\ the factors $10$, $100$, and $1000$ would give us $1$, $2$, and $3$ decimals respectively.

The LAS standard defines several classification codes, as listed in Table~\ref{tab:las-classes}.
\begin{table}
  \centering
\begin{tabular}{l|l}
  % \toprule
  % \rowcolor[gray]{.9}
  Code & Meaning \\ \midrule
  0 & never classified \\
  1 & unclassified \\
  2 & ground \\
  3 & low vegetation \\
  4 & medium vegetation \\
  5 & high vegetation \\
  6 & building \\
  7 & low point (noise) \\
  8 & \emph{reserved} \\
  9 & water \\
  % \bottomrule
\end{tabular}
\caption{The first 10 LAS classification code numbers. More codes exist, but they are not listed here.}
\label{tab:las-classes}
\end{table}
These codes are to be used as values for the classification field of a point record, and are intended to indicate the type of object a point belongs to.
Which classes are used strongly depends on the dataset at hand.
The codes $0$ and $1$ may appear ambiguous, but there is a clear distinction.
To be exact, the code $0$ is used for points that where never subject to a classication algorithm, whereas the code $1$ is used for points that where processed by an classification algorithm, but could not succesfully be assigned to a class.
It is possible to define your own classes using code ranges that are reserved for that purpose in the standard.
For example, the Dutch AHN3 dataset, which is disseminated in the LAS format, uses the code $26$ for an `artefact' class that includes infrastructural works such as bridges and viaducts (see Figure~\ref{fig:ahn3}).
\begin{figure}
  \includegraphics[width=\linewidth]{figs/ahn3.png}
  \caption{Classification codes used in the AHN3 dataset.}
  \label{fig:ahn3}
\end{figure}

%TODO: some remarks about user-defined classification codes

% The standard technically allows for additional user-defined attributes
Finally, a compressed variant of the LAS format, dubbed the LAZ format, exists.
While it is not maintained by an `official' organisation like the LAS standard, it is an open standard and it is widely used, especially for very big dataset.
Through the use of lossless compression algorithms, a LAZ file can be packed into a fraction of the storage space required for the equivalent LAS file without any loss of information.
The LAZ format closely resembles the LAS format, \ie\ the header and the structure of the point records are virtually identical.
In a LAZ file the point records are grouped in blocks of 50,000 records each.
Each block is individually compressed, which makes it possible to partially decompress only the needed blocks from a file (instead of always needing to compress the whole file).
This can save a lot of time if only a few point from a huge point cloud are needed.
Notice that the effectiveness of the compression algorithms depends on the similarity in information between subsequent point records.
Typically information is quite similar for points that are close to each other in space.
Therefore, a greater compression factor can often be achieved after spatially sorting the points.

\section{Thinning}
A point cloud with fewer points is easier to manage and quicker to process.
Therefore a point cloud is sometimes \emph{thinned}, which simply means that a portion of the points is discarded and not used for processing.
Commonly encountered thinning methods in practice are
\begin{description}
  \item[random] randomly remove a given percentage of the points,
  \item[nth-point] iterate throught the points and keep only the first point for every $n$ points. For example a dataset with 1000 points is reduced to 100 points if $n=10$. This is the quickest thinning method.
  \item[grid] Overlay a 2D or 3D regular grid over the point cloud and keep one point per grid cell. That can be one of the original points, an average of those, or the exact center of the cell. The thinning factor depends on the chosen cell-size. Notice that the result is often a point cloud with a homogeneous point density on all surfaces (only on the horizontal surfaces if a 2D grid is used).
\end{description}
See Figure~\ref{fig:randvsgrid} for a comparison between random thinning and grid thinning.
\begin{figure}[htb]
  \centering
  \begin{subfigure}[b]{0.95\linewidth}
    \centering
    \includegraphics[width=\textwidth]{figs/rand01.png}
    \caption{random thinning}
  \end{subfigure}

  \begin{subfigure}[b]{0.95\linewidth}
    \centering
    \includegraphics[width=\textwidth]{figs/voxel08m.png}
    \caption{3D grid thinning}
  \end{subfigure}
\caption{Comparison of two thinning methods. The thresholds were chosen such that the number of remaining points is approximately the same.}
\label{fig:randvsgrid}
\end{figure}

From Lesson 08 you undoubtedly remember that TIN simplification has a somewhat similar objective: data reduction. 
However, for a given number of resulting points TIN simplification yields a higher quality end result because it only removes points that are deemed unimportant.
Thinning methods on the other hand do not consider the `importance' of a point in any way, and might discard a lot of potentially meaningful details.
So why bother with thinning? The answer is that thinning methods are a lot faster since they do not require something like a computationally expensive triangulation.
Especially in scenarios where the point density is very high and the available time is limited, thinning can be useful.


\section{Outlier detection}
Recall from Lesson 02 that outliers are points that have a large error in their coordinates.
Outliers are typically located far away from the terrain surface and often occur in relatively low densities.
Outlier detection aims to detect and remove outliers and is a common processing step for point clouds.

Most outlier detection methods revolve around analysing the local neighbourhood of a point.
The neighbourhood can be defined using a $k$-nearest neigbhour (knn) search, a fixed radius search, or by superimposing a regular grid on the point cloud and finding the points that are in the same grid-cell.
The points that are determined to be in the neighbourhood of a point of interest $\mathbf{p}$ are used to determine whether $\mathbf{p}$ is an outlier or not.

The underlying assumption of most outlier detection methods is that an outlier is often an isolated point, \ie\ there are not many points in its neighbourhood. We distinguish the following outlier detection methods (see also Figure~\ref{fig:outlier-detection}):
\begin{figure}[htb]
  \centering
  \begin{subfigure}[b]{0.3\linewidth}
    \centering
    \includegraphics[width=\textwidth]{figs/radius-count.pdf}
    \caption{radius count}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \centering
    \includegraphics[width=\textwidth]{figs/grid-count.pdf}
    \caption{grid count}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \centering
    \includegraphics[width=\textwidth]{figs/knn-distance.pdf}
    \caption{knn distance ($k=3$)}
  \end{subfigure}
\caption{Three outlier detection methods based on local point density. The red point is an outlier, whereas the blue point is an inlier.}
\label{fig:outlier-detection}
\end{figure}

\begin{description}
  \item[radius count] count the number of points that are within a fixed radius from $\mathbf{p}$. If the count is lower than a given threshold, $\mathbf{p}$ is marked as an outlier.
  \item[grid count] Superimpose a grid on the point cloud and count for each grid-cell the number of points. If the count is lower than a given threshold, the points inside the corresponding grid cell are marked as outliers. Sometimes the neighbourhood is extended with adjacent grid cells. The grid method has the advantage that it can be used with the spatial streaming paradigm, that was explained in Lesson 10.
  \item[knn distance] Find the $k$ nearest neigbhours of $\mathbf{p}$, \eg\ using a kd-tree (see Lesson 10), and compute the mean or median of the distances between $\mathbf{p}$ and its neigbhours. If this value is above a given threshold, $\mathbf{p}$ is marked as an outlier.
\end{description}

%TODO note on sonar cleaning method with DT from Lars Arge \citep{Arge10}
These methods generally work well if the outliers are isolated.
However, in some cases this assumption does not hold.
For example in case of a point cloud derived from multi-beam echo sounding\footnote{See Lesson 02.} a common issue is the occurence of (shoals of) fish. 
These fish cause large groups of points that are clustered closely together above the seafloor.
These are not isolated points since each outlier will have plenty of other points nearby.
A possible solution is to construct a TIN of all points and to `cut' the relatively long edges that connect the outlier clusters to the seafloor.
This splits the TIN into several smaller TINs, and the largest of those should then be the seafloor surface without the outliers. 
Figure~\ref{fig:mbes} gives an example.
\begin{figure}[htb]
  \centering
  \begin{subfigure}[b]{0.4\linewidth}
    \centering
    \includegraphics[width=\textwidth]{figs/mbes_cleaning_before.png}
    \caption{before outlier detection}
  \end{subfigure}
  \qquad%
  \begin{subfigure}[b]{0.4\linewidth}
    \centering
    \includegraphics[width=\textwidth]{figs/mbes_cleaning_after.png}
    \caption{after outlier detection}
  \end{subfigure}
\caption{Outlier detection in a multi-beam echo sounding dataset using a TIN \citep{Arge10}.}
\label{fig:mbes}
\end{figure}

%TODO: add figures for outlier detection methods


%%%%%%%%%%%%%%%%%%%%
%

\section{Ground filtering}
Ground filtering is a common processing-step for point clouds.
It involves classifying the points of a point cloud into ground points and non-ground points.
Ground points are those points that are part of the bare-earth surface of the earth, thus excluding vegetation and man-made structures such as buildings and cars.
The ground points can then be used to generate a DTM, usually as a TIN or a raster.
Or, the non-ground points can be used as input for another classifier, \eg\ to classify buildings and vegetation possibly using a region growing algorithm (see Section~\ref{sec:segmentation}).

Ground filtering methods are typically based on the assumptions that 1) the ground is a continuous surface without sudden elevation jumps, and 2) for a given 2D neighborhood, the ground points are the ones with the lowest elevation.
This is reasonable because non-ground points are typically measurements from objects above the ground such as trees, street furniture and buildings (see \eg\ Figure~\ref{fig:axelsson:profiles}).

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.8\textwidth]{figs/axelsson-profiles.png}
  \caption{Cross-section of a terrain with lamp posts and trees before (top) and after (bottom) ground filtering \citep{axelsson2000generation}.}
 \label{fig:axelsson:profiles}
\end{figure}

Notice that the resulting bare-earth model may thus have holes where these non-ground objects used to be.
If needed, these holes can be filled in a subsequent processing step involving spatial interpolation.

\subsection{TIN refinement for ground filtering}
We will now discuss an effective ground filtering method that is based on the greedy insertion of ground points into a TIN\@.
Indeed, the same algorithmic paradigm of iterative TIN refinement that we saw earlier in Lesson 08 is used.
The algorithm consists of three main steps:
\begin{enumerate}
  \item construction of a rudimentary initial TIN,
  \item computation of two geometric properties for each point that is not already labelled as ground, and
  \item incremental insertion of points that pass a `ground test' based on the computed geometric properties.
\end{enumerate}
The latter two steps are repeated until all remaining points fail the ground test.

In the first step a rudimentary initial TIN is constructed from a number of points that have locally the lowest elevation and are spread somewhat evenly over the data extent.
These points are found by superimposing a 2D grid over the data extent and by selecting the lowest point for each grid cell (similar to grid thinning).
The cell-size of the grid should be chosen such that it is larger than the largest non-ground object (usually a building).
Thus, if the largest building has a footprint of 100x100m, the cellsize should be a bit larger, \eg\ 110m, so that it is guaranteed that each grid-cell has at least a few ground points.
Each point that is inserted into the TIN is considered to be a ground point.

In the second step two geometric properties are computed for each unclassified point.
These properties are based on the relation between the point $\mathbf{p}$ and the triangle in the current TIN that intersects its vertical projection. The two properties are illustrated in Figure~\ref{fig:ground-filtering:symbols}.
\begin{figure}[htb]
  \centering
  \begin{subfigure}[b]{0.3\linewidth}
    \centering
    \includegraphics[width=\textwidth]{figs/groundfilter-symbols.pdf}
    \caption{}
    \label{fig:ground-filtering:symbols}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \centering
    \includegraphics[width=\textwidth]{figs/groundfilter-ground.pdf}
    \caption{example ground point}
    \label{fig:ground-filtering:ground}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \centering
    \includegraphics[width=\textwidth]{figs/groundfilter-nonground.pdf}
    \caption{example non-ground point}
    \label{fig:ground-filtering:nonground}
  \end{subfigure}
\caption{Geometric properties for a point $\mathbf{p}$ in the method for ground filtering based on TIN refinement.}
\label{fig:ground-filtering}
\end{figure}
The first property, denoted $d$, is the perpendicular distance between the $\mathbf{p}$ and the triangle.
The second property, denoted $\alpha$, is the largest angle of the angles between the triangle and the three vectors that connect each vertex with $\mathbf{p}$. 
% TODO figure

In the ground test of the final step, it is simply checked for each point if its $d$ is below a given threshold $d_{max}$ and if its $\alpha$ is below a given threshold $\alpha_{max}$.
If this is indeed the case, the point is labelled as a ground point and inserted into the TIN.
Compare Figures~\ref{fig:ground-filtering:ground} and~\ref{fig:ground-filtering:nonground}.

Of course, if the triangles in the TIN change, the properties of the overlapping unclassified points need to be recomputed. 
When all remaining points fail the ground test, the algorithm terminates.
Figure~\ref{fig:axelsson} gives an example result.

\begin{figure}[htb]
  \centering
  \begin{subfigure}[b]{0.9\linewidth}
    \centering
    \includegraphics[width=\textwidth]{figs/axelsson-before.png}
    \caption{Before}
  \end{subfigure}

  \begin{subfigure}[b]{0.9\linewidth}
    \centering
    \includegraphics[width=\textwidth]{figs/axelsson-after.png}
    \caption{After}
  \end{subfigure}
\caption{Ground filtering \citep{axelsson2000generation}}
\label{fig:axelsson}
\end{figure}

\section{Segmentation}
\label{sec:segmentation}
Segmentation is the process of grouping point clouds into multiple homogeneous regions with similar properties.
% Precisely what similarity properties to use depends on your goal.
A popular application of segmentation is the detection of planar regions in a point cloud.
In this case the point cloud is segmented into groups of points that each correspond to a plane.
This can be helpful for something like building classification, if we assume that buildings consist of planar surfaces.

\begin{link-box}
  It explains two segmention techniques: RANSAC and region growing. They are explained in respectively
  \\ \\
  Section 2.3.2 from page 67 up to and including page 69, and \\
  Section 2.3.3 on pages 72--74.
  \\ \\
  \bibentry{Vosselman10}
  \\
  Online access: \url{https://tudelft.on.worldcat.org/oclc/843860784}
\end{link-box}




%%%%%%%%%%%%%%%%%%%%
%
\section{Notes \& comments}
% More information on the PLY format can be found online\footnote{\url{http://paulbourke.net/dataformats/ply/}}. Notice that the PLY format can also be used to store a 3D mesh.

% The full LAS specification is described in \citet{LAS13}, and \citet{Isenburg13} describes the details of the compressed LAZ format.

% \citet{axelsson2000generation} originally proposed the greedy TIN insertion algorithm for ground filtering.
% He also describes how to handle discontinuities in the terrain such as cliffs.
% It should be said that his paper is a bit scarce on details, and if you are interested in those you are better off reading some excerpts of the work of \citet{Lin14}. 

% A comparison with several other ground filtering methods can be found in the work of \citet{Meng10}.




%%%%%%%%%%%%%%%%%%%%
%
\section{Exercises}

% TODO : add exercices

\begin{enumerate}
  \item laba33
  \item aldkfj
\end{enumerate}
